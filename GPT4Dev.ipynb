{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT как персональный раб разработчика\n",
    "\n",
    "Используем большие языковые модели для автоматизации сложных задач.\n",
    "\n",
    "Для начала научимся использовать большие языковые модели программно. Я рекомендую посмотреть на библиотеку [LangChain](https://www.langchain.com/).\n",
    "\n",
    "> Если вы открыли код из Google Colab, вам нужно создать файл `config.json`, содержащий ключи для доступа к моделям, следующего вида:\n",
    "```json\n",
    "{\n",
    "    \"api_key\" : \"...\",\n",
    "    \"gigachain_auth\" : \"...\"\n",
    "}\n",
    "```\n",
    "\n",
    "Для поддержки модели Yandex GPT можно дополнительно установить библиотеку [`yandex_chain`](https://github.com/yandex-datasphere/yandex-chain), в которой чуть больше возможностей по работе с YandexGPT, чем в стандартной LangChain.\n",
    "\n",
    "Для начала, установим библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T21:11:39.978094Z",
     "iopub.status.busy": "2024-05-09T21:11:39.977188Z",
     "iopub.status.idle": "2024-05-09T21:12:11.190095Z",
     "shell.execute_reply": "2024-05-09T21:12:11.189080Z",
     "shell.execute_reply.started": "2024-05-09T21:11:39.978045Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting yandex_chain\n",
      "  Downloading yandex-chain-0.0.7.tar.gz (8.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting langchain\n",
      "  Downloading langchain-0.1.19-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests in /kernel/lib/python3.8/site-packages (from yandex_chain) (2.31.0)\n",
      "  Downloading langchain-0.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tenacity (from yandex_chain)\n",
      "  Downloading tenacity-8.3.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.8/dist-packages (from langchain) (5.3.1)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading SQLAlchemy-2.0.30-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Downloading aiohttp-3.9.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from langchain) (4.0.2)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Downloading dataclasses_json-0.6.5-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.9 (from langchain)\n",
      "  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting langchain-core<0.2,>=0.1.7 (from langchain)\n",
      "  Downloading langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.77 (from langchain)\n",
      "  Downloading langsmith-0.0.92-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/jupyter/.local/lib/python3.8/site-packages (from langchain) (1.24.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.8/dist-packages (from langchain) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /kernel/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (5.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.7.2)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading marshmallow-3.21.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.8/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.2)\n",
      "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-community<0.1,>=0.0.9 (from langchain)\n",
      "  Downloading langchain_community-0.0.37-py3-none-any.whl.metadata (8.7 kB)\n",
      "  Downloading langchain_community-0.0.36-py3-none-any.whl.metadata (8.7 kB)\n",
      "  Downloading langchain_community-0.0.35-py3-none-any.whl.metadata (8.7 kB)\n",
      "  Downloading langchain_community-0.0.34-py3-none-any.whl.metadata (8.5 kB)\n",
      "  Downloading langchain_community-0.0.33-py3-none-any.whl.metadata (8.5 kB)\n",
      "  Downloading langchain_community-0.0.32-py3-none-any.whl.metadata (8.5 kB)\n",
      "  Downloading langchain_community-0.0.31-py3-none-any.whl.metadata (8.4 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading langchain_community-0.0.30-py3-none-any.whl.metadata (8.4 kB)\n",
      "  Downloading langchain_community-0.0.29-py3-none-any.whl.metadata (8.3 kB)\n",
      "  Downloading langchain_community-0.0.28-py3-none-any.whl.metadata (8.3 kB)\n",
      "  Downloading langchain_community-0.0.27-py3-none-any.whl.metadata (8.2 kB)\n",
      "  Downloading langchain_community-0.0.26-py3-none-any.whl.metadata (8.2 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading langchain_community-0.0.25-py3-none-any.whl.metadata (8.1 kB)\n",
      "  Downloading langchain_community-0.0.24-py3-none-any.whl.metadata (8.1 kB)\n",
      "  Downloading langchain_community-0.0.23-py3-none-any.whl.metadata (8.1 kB)\n",
      "  Downloading langchain_community-0.0.22-py3-none-any.whl.metadata (8.1 kB)\n",
      "  Downloading langchain_community-0.0.21-py3-none-any.whl.metadata (8.1 kB)\n",
      "  Downloading langchain_community-0.0.20-py3-none-any.whl.metadata (8.1 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-core<0.2,>=0.1.7 (from langchain)\n",
      "  Downloading langchain_core-0.1.51-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.50-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.49-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.48-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.47-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.46-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.45-py3-none-any.whl.metadata (5.9 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading langchain_core-0.1.44-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.43-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.42-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.41-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.40-py3-none-any.whl.metadata (5.9 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading langchain_core-0.1.39-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.38-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.37-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.36-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.35-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.34-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.33-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting anyio<5,>=3 (from langchain-core<0.2,>=0.1.7->langchain)\n",
      "  Downloading anyio-4.3.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting langchain-core<0.2,>=0.1.7 (from langchain)\n",
      "  Downloading langchain_core-0.1.32-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.31-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.30-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.29-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.28-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.27-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.26-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.25-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.24-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.23-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.77 (from langchain)\n",
      "  Downloading langsmith-0.0.87-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2,>=0.1.7->langchain)\n",
      "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /kernel/lib/python3.8/site-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /kernel/lib/python3.8/site-packages (from requests->yandex_chain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /kernel/lib/python3.8/site-packages (from requests->yandex_chain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /kernel/lib/python3.8/site-packages (from requests->yandex_chain) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.8/site-packages (from requests->yandex_chain) (2024.2.2)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.0.3-cp38-cp38-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting sniffio>=1.1 (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting exceptiongroup>=1.0.2 (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain)\n",
      "  Downloading exceptiongroup-1.2.1-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langchain-0.1.0-py3-none-any.whl (797 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.0/798.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.5-py3-none-any.whl (28 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_community-0.0.20-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.1.23-py3-none-any.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langsmith-0.0.87-py3-none-any.whl (55 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading SQLAlchemy-2.0.30-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
      "Downloading anyio-4.3.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading greenlet-3.0.3-cp38-cp38-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (622 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.3/622.3 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading exceptiongroup-1.2.1-py3-none-any.whl (16 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Building wheels for collected packages: yandex_chain\n",
      "  Building wheel for yandex_chain (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for yandex_chain: filename=yandex_chain-0.0.7-py3-none-any.whl size=8135 sha256=b4d4b3ac503d58cbc0f91d734b91d983079a4973af89e8b0ee07d75508b748f5\n",
      "  Stored in directory: /tmp/xdg_cache/pip/wheels/a4/53/3b/048df8fcd6af62bff74038ebd156bc2f62be94fd4a7f9d655b\n",
      "Successfully built yandex_chain\n",
      "Installing collected packages: tenacity, sniffio, packaging, mypy-extensions, jsonpatch, greenlet, exceptiongroup, typing-inspect, SQLAlchemy, marshmallow, langsmith, anyio, aiohttp, langchain-core, dataclasses-json, langchain-community, langchain, yandex_chain\n",
      "\u001b[33m  WARNING: The script langsmith is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script langchain-server is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cfn-lint 0.33.2 requires jsonschema~=3.0, but you have jsonschema 4.21.1 which is incompatible.\n",
      "moto 1.3.14 requires idna<2.9,>=2.5, but you have idna 3.7 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed SQLAlchemy-2.0.30 aiohttp-3.9.5 anyio-4.3.0 dataclasses-json-0.6.5 exceptiongroup-1.2.1 greenlet-3.0.3 jsonpatch-1.33 langchain-0.1.0 langchain-community-0.0.20 langchain-core-0.1.23 langsmith-0.0.87 marshmallow-3.21.2 mypy-extensions-1.0.0 packaging-23.2 sniffio-1.3.1 tenacity-8.3.0 typing-inspect-0.9.0 yandex_chain-0.0.7\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install yandex_chain==0.0.7 langchain=="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот как просто можно организовать вызов языковой модели Yandex GPT из кода:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T21:13:42.224331Z",
     "iopub.status.busy": "2024-05-09T21:13:42.223487Z",
     "iopub.status.idle": "2024-05-09T21:13:42.268140Z",
     "shell.execute_reply": "2024-05-09T21:13:42.266928Z",
     "shell.execute_reply.started": "2024-05-09T21:13:42.224284Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "issubclass() arg 1 must be a class",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a4acd7d7d175>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mYandexGPT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGigaChat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'config.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/langchain_community/llms/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseLLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/langchain_core/language_models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from langchain_core.language_models.base import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mLanguageModelInput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mLanguageModelLike\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mLanguageModelOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/langchain_core/language_models/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m from langchain_core.messages import (\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mAnyMessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mBaseMessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/langchain_core/messages/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAIMessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAIMessageChunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m from langchain_core.messages.base import (\n\u001b[1;32m      5\u001b[0m     \u001b[0mBaseMessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/langchain_core/messages/ai.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mAIMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseMessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;34m\"\"\"Message from an AI.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pydantic/main.cpython-38-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.ModelMetaclass.__new__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pydantic/fields.cpython-38-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.fields.ModelField.infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pydantic/fields.cpython-38-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.fields.ModelField.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pydantic/fields.cpython-38-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.fields.ModelField.prepare\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pydantic/fields.cpython-38-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.fields.ModelField._type_analysis\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/typing.py\u001b[0m in \u001b[0;36m__subclasscheck__\u001b[0;34m(self, cls)\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_special\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_GenericAlias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__origin__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_special\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__origin__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__origin__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: issubclass() arg 1 must be a class"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import YandexGPT, GigaChat\n",
    "import json\n",
    "\n",
    "config = json.load(open('config.json'))\n",
    "\n",
    "GPT = YandexGPT(api_key = config['api_key'], temperature=0.01)\n",
    "GC = GigaChat(credentials=config['gigachain_auth'],verify_ssl_certs=False)\n",
    "\n",
    "print(GPT(\"Напиши сказку про мальчика, который любил JSON\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New pypi version: 0.1.9.3 (current: 0.1.8.1) | pip install -U g4f\n",
      "**Сказка о Маленьком Программисте и Чудесном JSON-Лесе**\n",
      "\n",
      "Жил-был маленький мальчик по имени Алекс в мире, где компьютеры и интернет стали неотъемлемой частью жизни. С самого детства Алекс увлекался программированием, и его самой большой страстью был JSON.\n",
      "\n",
      "Однажды, Алекс отправился в путешествие в Чудесный JSON-Лес. В этом лесу каждое дерево было как ключ в объекте, а каждый лист — как значение. Но для Алекса особенное волшебство крылось в том, что каждое дерево могло рассказывать ему свою уникальную историю.\n",
      "\n",
      "Алекс шагал между деревьями, и каждое из них радостно раскрывало перед ним свой JSON-файл. Он узнавал о прошлом и настоящем каждого дерева: какие плоды они давали, какие приключения им приходилось переживать. И маленький программист слушал их истории с увлечением.\n",
      "\n",
      "Однажды, Алекс наткнулся на особенно интересное дерево, которое рассказало ему о Золотом JSON-Ключе. Этот ключ считался самым ценным во всем лесу. Легенда гласила, что тот, кто найдет Золотой JSON-Ключ, сможет решить любую задачу и сделать свою жизнь по-настоящему удивительной.\n",
      "\n",
      "Алекс решился на поиск Золотого JSON-Ключа. Он путешествовал по лесу, изучая каждое дерево и анализируя каждый узел. В процессе своего путешествия, он сталкивался с разными вызовами и задачами, но благодаря своим знаниям о JSON, он с легкостью преодолевал все трудности.\n",
      "\n",
      "Наконец, после долгих поисков, Алекс обнаружил Золотой JSON-Ключ, который лежал в самом глубоком уголке леса. Когда он вставил этот ключ в свой виртуальный рюкзак, произошло нечто невероятное.\n",
      "\n",
      "Все деревья в Чудесном JSON-Лесе начали сверкать ярким светом, и лес наполнился музыкой. Алекс почувствовал, как его знания и умения стали еще более мощными. Теперь он мог создавать удивительные программы, решать сложнейшие задачи и делиться своими знаниями с другими.\n",
      "\n",
      "И так, с Золотым JSON-Ключом в руках, Алекс стал настоящим героем программирования. Он помогал другим, создавал инновационные проекты и делал мир вокруг себя лучше. Все это благодаря его любви к JSON и неутомимой жажде знаний.\n",
      "\n",
      "И хотя сказка о Маленьком Программисте и Чудесном JSON-Лесе может показаться вымышленной, она напоминает нам о том, что страсть к знаниям и умению преодолевать трудности способны сделать наш мир по-настоящему удивительным.\n"
     ]
    }
   ],
   "source": [
    "import g4f \n",
    "\n",
    "def GPT4(x):\n",
    "    response = g4f.ChatCompletion.create(\n",
    "    model=g4f.models.default,\n",
    "    messages=[{\"role\": \"user\", \"content\": x }])\n",
    "    return response\n",
    "\n",
    "res = GPT4('Придумай сказку про мальчика, который любил JSON')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем сделать что-то полезное:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Код\n",
      "2. Паскаль\n",
      "3. Си\n",
      "4. Ассемблер\n",
      "5. Делфи\n",
      "6. Бейсик\n",
      "7. Ява\n",
      "8. Питон\n",
      "9. С++\n",
      "10. Фортран\n"
     ]
    }
   ],
   "source": [
    "res = GPT(\"Придумай 10 смешных кличек для собаки программиста\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основные приёмы промптинга\n",
    "\n",
    "Важно, чтобы модель получила чёткие и понятные инструкции по тому, что же ей нужно сделать.\n",
    "\n",
    "#### Используем ограничители"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Чтобы модель правильно поняла ваш запрос, он должен быть четким и конкретным.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Вы должны выразить то, что вы хотите, чтобы модель сделала, \n",
    "предоставив инструкции, которые максимально ясны и конкретны.\n",
    "Это направит модель на желаемый результат и уменьшит вероятность\n",
    "получения несвязанных или неправильных ответов. Не путайте\n",
    "написание четкого запроса с написанием короткого запроса. \n",
    "Во многих случаях более длинные запросы обеспечивают большую ясность \n",
    "и контекст для модели, что может привести к более подробным \n",
    "и соответствующим ответам.\n",
    "\"\"\"\n",
    "\n",
    "instr = \"\"\"\n",
    "    Сократи текст, выделенный тройными обратными\n",
    "    кавычками, до одного предложения. Выведи в качестве результата\n",
    "    одно предложение, содержащее главную мысль текста.\n",
    "    ```{}```\"\"\"\n",
    "\n",
    "res = GPT(instr.format(text))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Чтобы модель правильно поняла ваш запрос, он должен быть четким и конкретным.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Сократи текст, выделенный тройными обратными\n",
    "    кавычками, до одного предложения. Выведи в качестве результата\n",
    "    одно предложение, содержащее главную мысль текста.\n",
    "    ```{text}```\"\"\",\n",
    "    input_variables=[\"text\"],\n",
    ")\n",
    "\n",
    "res = GPT(prompt.format(text=text))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Используем структурированный вывод"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"кличка\": \"Код\",\n",
      "    \"порода\": \"Такса\",\n",
      "    \"окрас\": \"Белый\",\n",
      "    \"пол\": \"Мужской\",\n",
      "    \"характер\": \"Умный, спокойный, добрый\"\n",
      "  },\n",
      "  {\n",
      "    \"кличка\": \"Паттерн\",\n",
      "    \"порода\": \"Хаски\",\n",
      "    \"окрас\": \"Черно-белый\",\n",
      "    \"пол\": \"Женский\",\n",
      "    \"характер\": \"Энергичный, дружелюбный, любознательный\"\n",
      "  },\n",
      "  {\n",
      "    \"кличка\": \"Фейл\",\n",
      "    \"порода\": \"Бультерьер\",\n",
      "    \"окрас\": \"Рыжий\",\n",
      "    \"пол\": \"Мужской\",\n",
      "    \"характер\": \"Амбициозный, упрямый, игривый\"\n",
      "  },\n",
      "  {\n",
      "    \"кличка\": \"Скрэтч\",\n",
      "    \"порода\": \"Пудель\",\n",
      "    \"окрас\": \"Черный\",\n",
      "    \"пол\": \"Мужской\",\n",
      "    \"характер\": \"Аккуратный, внимательный, общительный\"\n",
      "  },\n",
      "  {\n",
      "    \"кличка\": \"Глюкодром\",\n",
      "    \"порода\": \"Бигль\",\n",
      "    \"окрас\": \"Коричневый\",\n",
      "    \"пол\": \"Мужской\",\n",
      "    \"характер\": \"Любознательный, игривый, энергичный\"\n",
      "  },\n",
      "  {\n",
      "    \"кличка\": \"Виртуоз\",\n",
      "    \"порода\": \"Ротвейлер\",\n",
      "    \"окрас\": \"Оранжевый\",\n",
      "    \"пол\": \"Мужской\",\n",
      "    \"характер\": \"Смелый, решительный, преданный\"\n",
      "  },\n",
      "  {\n",
      "    \"кличка\": \"Бэкенд\",\n",
      "    \"порода\": \"Лабрадор\",\n",
      "    \"окрас\": \"Золотистый\",\n",
      "    \"пол\": \"Мужской\",\n",
      "    \"характер\": \"Дружелюбный, игривый, терпеливый\"\n",
      "  },\n",
      "  {\n",
      "    \"кличка\": \"Хекслет\",\n",
      "    \"порода\": \"Джек-рассел-терьер\",\n",
      "    \"окрас\": \"Трехцветный\",\n",
      "    \"пол\": \"Мужской\",\n",
      "    \"характер\": \"Активный, жизнерадостный, настойчивый\"\n",
      "  },\n",
      "  {\n",
      "    \"кличка\": \"Дебаггер\",\n",
      "    \"порода\": \"Сенбернар\",\n",
      "    \"окрас\": \"Бело-серый\",\n",
      "    \"пол\": \"Мужской\",\n",
      "    \"характер\": \"Спокойный, уравновешенный, заботливый\"\n",
      "  },\n",
      "  {\n",
      "    \"кличка\": \"Линукс\",\n",
      "    \"порода\": \"Йоркширский терьер\",\n",
      "    \"окрас\": \"Серый\",\n",
      "    \"пол\": \"Мужской\",\n",
      "    \"характер\": \"Игривый, любознательный, умный\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "GPT.temperature=0.5\n",
    "res = GPT(\"Придумай 10 смешных кличек для собаки программиста и выведи результат в формате JSON\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы убедиться в том, что вывод соответствуюет некоторому формату, используют выходные парсеры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Код,Код,Код\"', '\"Патч,Патч,Патч\"', '\"Коммит,Коммит,Коммит\"', '\"Скомпилировать,Скомпилировать,Скомпилировать\"', '\"Дебажить,Дебажить,Дебажить\"', '\"Отладка,Отладка,Отладка\"', '\"Отладка,Отладка,Отладка\"', '\"Отладка,Отладка,Отладка\"', '\"Отладка,Отладка,Отладка\"', '\"Отладка,Отладка,Отладка\"']\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "csv_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "GPT.temperature = 0.01\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Придумай 10 смешных {subject}, которые бы были оригинальными. {format_instructions}. Используй формат CSV в одну строку\",\n",
    "    input_variables=[\"subject\"],\n",
    "    output_parser=csv_parser,\n",
    "    partial_variables={ \"format_instructions\" : csv_parser.get_format_instructions() }\n",
    ")\n",
    "res = GPT(prompt.format(subject=\"кличек для собаки программиста\"))\n",
    "print(csv_parser.parse(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэкспериментируем с температурой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 -> ['1. Код\\n2. Скрипт\\n3. Функция\\n4. Переменная\\n5. Модуль\\n6. Константа\\n7. Оператор\\n8. Тип данных\\n9. Класс\\n10. Метод']\n",
      "0.5 -> ['1.\\tКодзи\\n2.\\tТьюринг\\n3.\\tБайт\\n4.\\tСкретч\\n5.\\tАссемблер\\n6.\\tСид\\n7.\\tПаскаль\\n8.\\tПерл\\n9.\\tPython\\n10.\\tБит']\n",
      "0.9 -> ['Глюкодром', 'Коддеус', 'Паскаль', 'Ай-Ти', 'Вай фай', 'Пиксель', 'Бейсик', 'Ассемблер', 'Убунту', 'Виндоус']\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "for t in [0.1, 0.5, 0.9]:\n",
    "    GPT.temperature = t\n",
    "    res = GPT(prompt.format(subject=\"кличек для собаки программиста\"))\n",
    "    time.sleep(1)\n",
    "    print(f\"{t} -> {csv_parser.parse(res)}\")\n",
    "\n",
    "GPT.temperature = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Используем условия\n",
    "\n",
    "Попробуем использовать GPT для выделения последовательности инструкций из текста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Шаг 1 - Откройте приложение \"Едадил\".\n",
      "Шаг 2 - Введите в поисковой строке \"яичный омлет\".\n",
      "Шаг 3 - Нажмите на кнопку \"Показать\".\n",
      "Шаг 4 - Прокрутите страницу вниз до раздела \"Где купить\".\n",
      "Шаг 5 - Выберите ближайший к вам магазин, где продается яичный омлет.\n",
      "Шаг 6 - Нажмите на кнопку \"Показать на карте\".\n",
      "Шаг 7 - Проложите маршрут до магазина и посмотрите, сколько времени займет дорога.\n",
      "Шаг 8 - Купите яичный омлет в магазине и приготовьте его дома.\n",
      "Шаг 9 - Наслаждайтесь вкусным завтраком!\n"
     ]
    }
   ],
   "source": [
    "text1 = \"\"\"\n",
    "Чтобы приготовить омлет, сначала надо взять яйца. Разбиваем их молотком, затем\n",
    "аккуратно извлекаем осколки скорлупы. Затем добавляем соли. В конце кладем масло на\n",
    "сковородку, и выливаем туда яичную смесь.\n",
    "\"\"\"\n",
    "\n",
    "text2 = \"\"\"\n",
    "Яичный омлет - это прекрасный завтрак! Вам обязательно стоит его попробовать, если\n",
    "раньше никогда не пробовали!\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Тебе будет дан текст, выделенный тройными обратными кавычками, в котором содержится\n",
    "    последовательность инструкций. Перепиши их\n",
    "    в виде последовательных шагов в таком формате:\n",
    "    Шаг 1 - ...\n",
    "    Шаг 2 - ...\n",
    "    ...\n",
    "    Шаг N - ... \n",
    "    ```{text}```\"\"\",\n",
    "    input_variables=[\"text\"],\n",
    ")\n",
    "\n",
    "res = GPT(prompt.format(text=text2))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наблюдаете галлюцинации, при попытке выделить инструции из `text2`?\n",
    "\n",
    "Чтобы этого избежать, слегка модифицируем инструкцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Инструкций нет\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Тебе будет дан текст, выделенный тройными обратными кавычками. Если\n",
    "    в тексте содержится последовательность инструкций, перепиши их\n",
    "    в виде последовательных шагов в таком формате:\n",
    "    Шаг 1 - ...\n",
    "    Шаг 2 - ...\n",
    "    ...\n",
    "    Шаг N - ...\n",
    "\n",
    "    Если в тексте нет конкретных инструкций, напиши \"Инструкций нет\". \n",
    "    ```{text}```\"\"\",\n",
    "    input_variables=[\"text\"],\n",
    ")\n",
    "\n",
    "res = GPT(prompt.format(text=text2))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few-Shot\n",
    "\n",
    "Few-Shot Learning - это когда мы пытаемся \"научить\" модель прямо в запросе, дав ей несколько примеров. Это может быть полезно как для задания точного формата вывода, так и для формулирования самого задания, например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Искренность - это как маленький ручеек, который рождается\n",
      "в глубине земли, и пробивается на поверхность. Этот ручеек\n",
      "не может быть большим, но он всегда будет течь, и будет\n",
      "чистым и свежим.\n",
      "\n",
      "[Ребенок]: Расскажи мне о доброте.\n",
      "[Родитель]: Доброта - это как большое озеро, которое собирает\n",
      "все маленькие ручейки и речки, и становится еще больше.\n",
      "Это озеро никогда не переполняется, и всегда готово поделиться\n",
      "своей водой с другими.\n",
      "\n",
      "[Ребенок]: Расскажи мне о любви.\n",
      "[Родитель]: Любовь - это как солнце, которое светит на все вокруг,\n",
      "и согревает всех, кто находится рядом. Это солнце никогда не\n",
      "устает светить, и всегда будет дарить тепло и радость.\n",
      "\n",
      "[Ребенок]: Расскажи мне о мире.\n",
      "[Родитель]: Мир - это как большой лес, в котором живут все живые\n",
      "существа, и никто не мешает друг другу. В этом лесу всегда\n",
      "спокойно и безопасно, и каждый может найти свое место.\n",
      "\n",
      "[Ребенок]: Расскажи мне о счастье.\n",
      "[Родитель]: Счастье - это как радуга, которая появляется после дождя,\n",
      "и показывает, что все будет хорошо. Радуга всегда напоминает\n",
      "нам о том, что после трудностей и проблем всегда приходит\n",
      "радость и удовлетворение.\n",
      "\n",
      "[Ребенок]: Расскажи мне о надежде.\n",
      "[Родитель]: Надежда - это как маленький росток, который пробивается\n",
      "сквозь землю, и показывает, что даже в самые темные времена\n",
      "есть свет и надежда. Этот росток всегда будет напоминать нам\n",
      "о том, что мы можем преодолеть любые трудности и достичь\n",
      "своих целей.\n",
      "\n",
      "[Ребенок]: Расскажи мне о благодарности.\n",
      "[Родитель]: Благодарность - это как цветок, который расцветает после\n",
      "дождя, и показывает, что мы должны быть благодарны за все,\n",
      "что у нас есть. Этот цветок всегда будет напоминать нам о\n",
      "том, что мы должны ценить то, что имеем, и быть благодарными\n",
      "за каждый момент нашей жизни.\n",
      "\n",
      "[Ребенок]: Расскажи мне о прощении.\n",
      "[Родитель]: Прощение - это как река, которая уносит все обиды и\n",
      "разочарования, и позволяет нам двигаться вперед. Эта река\n",
      "всегда будет помогать нам забыть о прошлом, и сосредоточиться\n",
      "на будущем.\n",
      "\n",
      "[Ребенок]: Расскажи мне о понимании.\n",
      "[Родитель]: Понимание - это как мост, который соединяет две стороны\n",
      "разлада, и помогает нам найти общий язык. Этот мост всегда\n",
      "будет помогать нам понимать друг друга, и находить общий\n",
      "язык в любых ситуациях.\n",
      "\n",
      "[Ребенок]: Расскажи мне о уважении.\n",
      "[Родитель]: Уважение - это как лестница, которая ведет нас к вершине\n",
      "успеха, и помогает нам достичь наших целей. Эта лестница\n",
      "всегда будет помогать нам подниматься вверх, и достигать\n",
      "новых высот.\n",
      "\n",
      "[Ребенок]: Расскажи мне о терпимости.\n",
      "[Родитель]: Терпимость - это как дорога, которая ведет нас через трудности\n",
      "и препятствия, и помогает нам преодолевать их. Эта дорога\n",
      "всегда будет помогать нам двигаться вперед, и достигать\n",
      "своих целей.\n",
      "\n",
      "[Ребенок]: Расскажи мне о сострадании.\n",
      "[Родитель]: Сострадание - это как луч света, который освещает темные\n",
      "места, и помогает нам видеть ясно. Этот луч всегда будет\n",
      "помогать нам видеть правду, и находить выход из сложных\n",
      "ситуаций.\n",
      "\n",
      "[Ребенок]: Расскажи мне о дружбе.\n",
      "[Родитель]: Дружба - это как дерево, которое растет в нашем сердце, и\n",
      "дает нам силу и поддержку. Это дерево всегда будет помогать\n",
      "нам расти и развиваться, и находить поддержку в трудные\n",
      "времена.\n",
      "\n",
      "[Ребенок]: Расскажи мне о любви к себе.\n",
      "[Родитель]: Любовь к себе - это как звезда, которая светит в нашей душе,\n",
      "и помогает нам видеть наши собственные достоинства. Эта\n",
      "звезда всегда будет помогать нам ценить себя, и находить\n",
      "уверенность в своих силах.\n",
      "\n",
      "[Ребенок]: Расскажи мне о гармонии.\n",
      "[Родитель]: Гармония - это как музыка, которая звучит в нашей душе, и\n",
      "помогает нам чувствовать себя спокойно и умиротворенно.\n",
      "Эта музыка всегда будет помогать нам находить баланс и\n",
      "равновесие в нашей жизни.\n",
      "\n",
      "[Ребенок]: Расскажи мне о радости.\n",
      "[Родитель]: Радость - это как цветок, который расцветает в нашей душе, и\n",
      "помогает нам видеть красоту и счастье вокруг нас. Этот\n",
      "цветок всегда будет помогать нам находить удовольствие в\n",
      "каждом моменте нашей жизни.\n",
      "\n",
      "[Ребенок]: Расскажи мне о мире во всем мире.\n",
      "[Родитель]: Мир во всем мире - это как мечта, которая сияет в наших сердцах,\n",
      "и помогает нам стремиться к лучшему будущему. Эта мечта\n",
      "всегда будет помогать нам объединяться и работать вместе,\n",
      "чтобы достичь этой цели.\n",
      "\n",
      "[Ребенок]: Расскажи мне о счастье.\n",
      "[Родитель]: Счастье - это как радуга, которая появляется после дождя, и\n",
      "показывает, что все будет хорошо. Радуга всегда напоминает\n",
      "нам о том, что после трудностей и проблем всегда приходит\n",
      "радость и удовлетворение.\n",
      "\n",
      "[Ребенок]: Расскажи мне о надежде.\n",
      "[Родитель]: Надежда - это как маленький росток, который пробивается сквозь\n",
      "землю, и показывает, что даже в самые темные времена есть\n",
      "свет и надежда. Этот росток всегда будет напоминать нам о\n",
      "том, что мы можем преодолеть любые трудности и достичь\n",
      "своих целей.\n",
      "\n",
      "[Ребенок]: Расскажи мне о благодарности.\n",
      "[Родитель]: Благодарность - это как цветок, который расцветает после дождя,\n",
      "и показывает, что мы должны быть благодарны за все, что у\n",
      "нас есть. Этот цветок всегда будет напоминать нам о том,\n",
      "что мы должны ценить то, что имеем, и быть благодарными\n",
      "за каждый момент нашей жизни.\n"
     ]
    }
   ],
   "source": [
    "res = GPT(\"\"\"\n",
    "Пожалуйста, ответь на вопрос ребенка в похожем стиле, продолжив диалог:\n",
    "    \n",
    "[Ребенок]: Расскажи мне о терпеливости.\n",
    "[Родитель]: Терпеливость - это как бесконечная река, которая \n",
    "течет сквозь равнины, и никогда не заканчивается. Этой реке\n",
    "никогда не надоедает течь, потому что она всегда спокойна и\n",
    "умиротворена.\n",
    "    \n",
    "[Ребенок]: Расскажи мне об искренности.\n",
    "[Родитель]:\n",
    "\"\"\")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дайте модели время подумать!\n",
    "\n",
    "Языковые модели не могут рассуждать, как человек, гоняя мысли в голове \"взад-вперёд\". Модель всегда генерирует текст \"вперёд\", и \"рассуждает\" в процессе генерации. Поэтому важно инструктировать модель так, чтобы она могла \"рассуждать вслух\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"позитивность\": \"Использовать генеративный ИИ полезно, потому что это очень сильно ускоряет работу.\",\n",
      "    \"негативность\": \"Также, работая с ChatGPT, мы можем многому у него научиться.\"\n",
      "  },\n",
      "  {\n",
      "    \"позитивность\": \"Используя передовые технологии, мы будем современными и не отставать от прогресса.\",\n",
      "    \"негативность\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"позитивность\": \"\",\n",
      "    \"негативность\": \"Но есть риск, что мы при этом разучимся сами писать.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Использовать генеративный ИИ полезно, потому что это очень \n",
    "сильно ускоряет работу. Также, работая с ChatGPT, мы можем\n",
    "многому у него научиться. Используя передовые технологии,\n",
    "мы будем современными и не отставать от прогресса. Но есть риск,\n",
    "что мы при этом разучимся сами писать.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Тебе нужно сделать следующее:\n",
    "    1. Выдели умные мысли, которые содержатся в тексте ниже, \n",
    "    выделенном тройными обратными кавычками.\n",
    "    2. Построй список из всех умных мыслей\n",
    "    2. Для каждой умной мысли определи, является ли она позитивной\n",
    "    или негативной.\n",
    "    3. Выведи ответ в формате JSON, который содержит список\n",
    "    умных мыслей и их позитивность/негативность.\n",
    "    ```{text}```\"\"\",\n",
    "    input_variables=[\"text\"],\n",
    ")\n",
    "\n",
    "res = GPT(prompt.format(text=text))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Умные мысли:\n",
      "- Использовать генеративный ИИ полезно, потому что это очень сильно ускоряет работу.\n",
      "- Работать с ChatGPT, мы можем многому у него научиться.\n",
      "- Есть риск, что мы при этом разучимся сами писать.\n",
      "- Используя передовые технологии, мы будем современными и не отставать от прогресса.\n",
      "\n",
      "Позитивные мысли:\n",
      "- Использовать генеративный ИИ полезно, потому что это очень сильно ускоряет работу.\n",
      "- Работать с ChatGPT, мы можем многому у него научиться.\n",
      "\n",
      "Негативные мысли:\n",
      "- Есть риск, что мы при этом разучимся сами писать.\n",
      "\n",
      "Ответ:\n",
      "{\n",
      "  \"умные мысли\": [\n",
      "    \"Использовать генеративный ИИ полезно, потому что это очень сильно ускоряет работу.\",\n",
      "    \"Работать с ChatGPT, мы можем многому у него научиться.\",\n",
      "    \"Есть риск, что мы при этом разучимся сами писать.\",\n",
      "    \"Используя передовые технологии, мы будем современными и не отставать от прогресса.\"\n",
      "  ],\n",
      "  \"позитивные мысли\": [\n",
      "    \"Использовать генеративный ИИ полезно, потому что это очень сильно ускоряет работу.\",\n",
      "    \"Работать с ChatGPT, мы можем многому у него научиться.\"\n",
      "  ],\n",
      "  \"негативные мысли\": [\n",
      "    \"Есть риск, что мы при этом разучимся сами писать.\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Использовать генеративный ИИ полезно, потому что это очень \n",
    "сильно ускоряет работу. Также, работая с ChatGPT, мы можем\n",
    "многому у него научиться. Но есть риск,\n",
    "что мы при этом разучимся сами писать. Используя передовые технологии,\n",
    "мы будем современными и не отставать от прогресса. \n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Тебе нужно сделать следующее:\n",
    "    1. Выдели умные мысли, которые содержатся в тексте ниже, \n",
    "    выделенном тройными обратными кавычками.\n",
    "    2. Построй список из всех умных мыслей\n",
    "    2. Для каждой умной мысли определи, является ли она позитивной\n",
    "    или негативной.\n",
    "    3. Выведи ответ в формате JSON, который содержит список\n",
    "    умных мыслей и их позитивность/негативность.\n",
    "    Используй следующий формат:\n",
    "    Текст: <исходный текст с мыслями>\n",
    "    Умные мысли: <список умных мыслей>\n",
    "    Позитивные мысли: <список позитивных мыслей>\n",
    "    Негативные мысли: <список негативных мыслей>\n",
    "    \n",
    "    Вот текст, с которым тебе надо работать:\n",
    "    ```{text}```\"\"\",\n",
    "    input_variables=[\"text\"],\n",
    ")\n",
    "\n",
    "GPT.temperature=0.01\n",
    "res = GPT(prompt.format(text=text))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Проверка решения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Решение студента верное. Общая стоимость уборки составляет 2800 рублей.\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Тебе необходимо проверить решение задачи по математике студентом. Напиши, правильное\n",
    "ли решение студента или нет.\n",
    "\n",
    "Задача:\n",
    "Необходимо посчитать стоимость уборки в доме площадью 20 кв.метров. \n",
    "Стоимость уборки складывается из:\n",
    "- приезд уборщика - 200 руб.\n",
    "- мытьё полов - 100 руб. за кв. метр.\n",
    "- уборка кухни - 500 руб.\n",
    "- чистка полов - 50 руб. за кв. метр.\n",
    "\n",
    "Решение студента:\n",
    "{solution}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"solution\"],\n",
    ")\n",
    "\n",
    "correct = \"\"\"\n",
    "Стоимость уборки:\n",
    "- приезд уборщика - 200 руб.\n",
    "- мытьё полов - 100 руб. * 20 кв. метров = 2000 руб.\n",
    "- уборка кухни - 500 руб.\n",
    "- чистка полов - 50 руб. * 20 кв. метров = 1000 руб.\n",
    "Общая стоимость: 200 руб. + 2000 руб. + 500 руб. + 1000 руб. = 3700 руб.\n",
    "Ответ: 3700 руб.\n",
    "\"\"\"\n",
    "\n",
    "incorrect = \"\"\"\n",
    "Стоимость уборки:\n",
    "- приезд уборщика - 200 руб.\n",
    "- мытьё полов - 100 руб. * 20 кв. метров = 2000 руб.\n",
    "- уборка кухни - 500 руб.\n",
    "- чистка полов - 5 руб. * 20 кв. метров = 100 руб.\n",
    "Общая стоимость: 200 руб. + 2000 руб. + 500 руб. + 100 руб. = 2800 руб.\n",
    "Ответ: 2800 руб.\n",
    "\"\"\"\n",
    "\n",
    "print(GC(prompt.format(solution=incorrect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Хорошо, давайте посчитаем стоимость уборки в доме площадью 20 кв. метров по шагам:\n",
      "\n",
      "1. **Приезд уборщика:**\n",
      "   - Стоимость приезда уборщика: 200 руб.\n",
      "\n",
      "2. **Мытье полов:**\n",
      "   - Площадь дома: 20 кв. м.\n",
      "   - Стоимость мытья полов: 100 руб. за кв. м.\n",
      "   - Общая стоимость мытья полов: \\(20 \\, \\text{кв. м.} \\times 100 \\, \\text{руб. / кв. м.}\\)\n",
      "\n",
      "3. **Уборка кухни:**\n",
      "   - Стоимость уборки кухни: 500 руб.\n",
      "\n",
      "4. **Чистка полов:**\n",
      "   - Площадь дома: 20 кв. м.\n",
      "   - Стоимость чистки полов: 50 руб. за кв. м.\n",
      "   - Общая стоимость чистки полов: \\(20 \\, \\text{кв. м.} \\times 50 \\, \\text{руб. / кв. м.}\\)\n",
      "\n",
      "Теперь сложим все эти стоимости, чтобы получить общую стоимость уборки:\n",
      "\n",
      "\\[ \\text{Общая стоимость} = \\text{Приезд уборщика} + \\text{Мытье полов} + \\text{Уборка кухни} + \\text{Чистка полов} \\]\n",
      "\n",
      "Подставим значения и рассчитаем:\n",
      "\n",
      "\\[ \\text{Общая стоимость} = 200 \\, \\text{руб.} + (\\text{Площадь} \\times 100 \\, \\text{руб. / кв. м.}) + 500 \\, \\text{руб.} + (\\text{Площадь} \\times 50 \\, \\text{руб. / кв. м.}) \\]\n",
      "\n",
      "\\[ \\text{Общая стоимость} = 200 + (20 \\times 100) + 500 + (20 \\times 50) \\]\n",
      "\n",
      "После подсчетов вы получите общую стоимость уборки в данном доме.\n"
     ]
    }
   ],
   "source": [
    "res = GPT4(\"\"\"\n",
    "Пожалуйста, реши по шагам следующую задачу:\n",
    "Необходимо посчитать стоимость уборки в доме площадью 20 кв.метров. \n",
    "Стоимость уборки складывается из:\n",
    "- приезд уборщика - 200 руб.\n",
    "- мытьё полов - 100 руб. за кв. метр.\n",
    "- уборка кухни - 500 руб.\n",
    "- чистка полов - 50 руб. за кв. метр.\n",
    "\"\"\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **Приезд уборщика:** 200 руб.\n",
      "\n",
      "2. **Мытьё полов:** 100 руб. * 20 кв. метров = 2000 руб.\n",
      "\n",
      "3. **Уборка кухни:** 500 руб.\n",
      "\n",
      "4. **Чистка полов:** 50 руб. * 20 кв. метров = 1000 руб.\n",
      "\n",
      "Общая стоимость: 200 руб. + 2000 руб. + 500 руб. + 1000 руб. = 3700 руб.\n",
      "\n",
      "**Ответ: 3700 руб.**\n",
      "\n",
      "Теперь сравним с решением студента:\n",
      "\n",
      "Студент неверно посчитал стоимость чистки полов. Вместо 5 руб. за кв. метр он написал 50 руб. за кв. метр. Верное значение - 5 руб. за кв. метр. Итак, ошибка студента в расчете стоимости чистки полов.\n",
      "\n",
      "**Итог: Решение студента неверно. Ошибка в стоимости чистки полов.**\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Тебе необходимо проверить решение задачи по математике студентом, которое приведено\n",
    "ниже в тройных обратных кавычках. Напиши, правильное\n",
    "ли решение студента или нет. Тебе необходимо сделать следующее:\n",
    "1. Сначала, реши задачу самостоятельно и выведи пошаговое решение.\n",
    "2. Сравни решение студента с твоим решением и скажи, правильно ли\n",
    "решение студента.\n",
    "Не принимай решения о том, правильно ли студент решил задачу, пока не \n",
    "решишь её самостоятельно.\n",
    "В качестве ответа представь своё решение и напиши, правильно ли студент\n",
    "решил задачу, и где он ошибся.\n",
    "\n",
    "Задача:\n",
    "Необходимо посчитать стоимость уборки в доме площадью 20 кв.метров. \n",
    "Стоимость уборки складывается из:\n",
    "- приезд уборщика - 200 руб.\n",
    "- мытьё полов - 100 руб. за кв. метр.\n",
    "- уборка кухни - 500 руб.\n",
    "- чистка полов - 50 руб. за кв. метр.\n",
    "\n",
    "Решение студента:\n",
    "```{solution}```\n",
    "Напоминаю, что тебе нужно самой решить задачу, и потом сравнить своё решение с решением студента.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"solution\"],\n",
    ")\n",
    "\n",
    "correct = \"\"\"\n",
    "Стоимость уборки:\n",
    "- приезд уборщика - 200 руб.\n",
    "- мытьё полов - 100 руб. * 20 кв. метров = 2000 руб.\n",
    "- уборка кухни - 500 руб.\n",
    "- чистка полов - 50 руб. * 20 кв. метров = 1000 руб.\n",
    "Общая стоимость: 200 руб. + 2000 руб. + 500 руб. + 1000 руб. = 3700 руб.\n",
    "Ответ: 3700 руб.\n",
    "\"\"\"\n",
    "\n",
    "incorrect = \"\"\"\n",
    "Стоимость уборки:\n",
    "- приезд уборщика - 200 руб.\n",
    "- мытьё полов - 100 руб. * 20 кв. метров = 2000 руб.\n",
    "- уборка кухни - 500 руб.\n",
    "- чистка полов - 5 руб. * 20 кв. метров = 100 руб.\n",
    "Общая стоимость: 200 руб. + 2000 руб. + 500 руб. + 100 руб. = 2800 руб.\n",
    "Ответ: 2800 руб.\n",
    "\"\"\"\n",
    "\n",
    "print(GPT4(prompt.format(solution=incorrect)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Итеративная разработка промптов\n",
    "\n",
    "Очень важный момент в промпт-инжиниринге - почти никогда хороший результат не получается с первого раза! Имеет смысл пробовать, изменять что-то в промпте, пока результат не будет достигнут! Также помогает экспериментировать с температурой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Xiaomi Mi 9\" - смартфон с процессором SnapDragon 855, дисплеем Samsung AMOLED 6.39 дюйма, камерой 48 МП SONY, фронтальной камерой 20 МП и тремя объективами 177 градусов. Он имеет стекло Corning Gorilla Glass 6, ОС MIUI 10, память 6 ГБ + 128 ГБ и разрешение 2340x1080 FHD+ с яркостью 600 нит и 430 нит.\n"
     ]
    }
   ],
   "source": [
    "techspec = \"\"\"\n",
    "Название: Xiaomi Mi 9\n",
    "Процессор: SnapDragon 855\n",
    "Зарядка: 20Вт беспроводная\n",
    "Дисплей: Samsung AMOLED, 6.39 дюймов\n",
    "Фото: 48 Мп SONY\n",
    "Фронтальная камера: 20 Мп\n",
    "Объективы: 3 шт., 177 град. широкоугольный\n",
    "Стекло: Corning Gorilla Glass 6\n",
    "ОС: MIUI 10\n",
    "Память: 6Гб + 128 Гб\n",
    "Разрешение: 2340 x 1080 FHD+ 403 PPI\n",
    "Яркость: 600 нит (HBM) / 430 нит (тип)\n",
    "\"\"\"\n",
    "\n",
    "template = \"\"\"\"\n",
    "Ты должен помочь отделу маркетинга сформировать привлекательное описание\n",
    "модели сотового телефона для потребителя. Описание приводится\n",
    "ниже в тройных обратных кавычках:\n",
    "```{techspec}```\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"techspec\"],\n",
    ")\n",
    "\n",
    "GPT.temperature = 0.4\n",
    "res = GPT(prompt.format(techspec=techspec))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Регулируем длину и целевую аудиторию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Xiaomi Mi 9 – это смартфон, который создан специально для тех, кто любит путешествовать. Он оснащен процессором SnapDragon 855, который обеспечивает высокую производительность и быстродействие. Также стоит отметить, что смартфон имеет беспроводную зарядку на 20 Вт, что позволяет заряжать его быстро и удобно.\n",
      "\n",
      "Xiaomi Mi 9 оснащен 6,39-дюймовым дисплеем Samsung AMOLED с разрешением 2340x1080 пикселей и яркостью до 600 нитов. Этот дисплей обеспечивает яркие и насыщенные цвета, а также имеет широкий угол обзора.\n",
      "\n",
      "Но самое главное в Xiaomi Mi 9 - это его камера. Он оснащен 48-мегапиксельной камерой SONY, которая позволяет делать фотографии высокого качества. Кроме того, смартфон имеет фронтальную камеру на 20 мегапикселей, которая позволяет делать качественные селфи.\n",
      "\n",
      "Также стоит отметить, что Xiaomi Mi 9 имеет три объектива с углом обзора 177 градусов, что позволяет делать фотографии с широким углом обзора. Кроме того, смартфон оснащен стеклом Corning Gorilla Glass 6, которое обеспечивает защиту от царапин и повреждений.\n",
      "\n",
      "Наконец, Xiaomi Mi 9 работает на операционной системе MIUI 10, которая обеспечивает быстрый и удобный интерфейс. Смартфон также имеет 6 ГБ оперативной памяти и 128 ГБ встроенной памяти, что позволяет хранить большое количество фотографий и файлов.\"\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\"\n",
    "Ты должен помочь отделу маркетинга сформировать подробное \n",
    "привлекательное описание модели сотового телефона для потребителя, состоящее\n",
    "из трех абзацев текста.\n",
    "Необходимо сосредоточиться на его преимуществах для фотографов, которые\n",
    "любят путешествовать. Описание приводится\n",
    "ниже в тройных обратных кавычках:\n",
    "```{techspec}```\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"techspec\"],\n",
    ")\n",
    "\n",
    "GPT.temperature = 0.4\n",
    "res = GPT(prompt.format(techspec=techspec))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основные приёмы использования \n",
    "\n",
    "1. Генерация текста по данным (экспансия)\n",
    "2. Извлечение данных из текста (экстракция)\n",
    "3. Суммаризация текста\n",
    "4. Десуммаризация текста\n",
    "5. Переписывание текста (тональность, акцент)\n",
    "6. Преобразование текста (перевод)\n",
    "\n",
    "### Пример\n",
    "\n",
    "Рассмотрим пример суммаризации множества отзывов, чтобы можно было охватить их одним взглядом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Отзыв о Макдональдсе: быстрое обслуживание, но невкусная еда.\n",
      "В котлете нашли что-то твердое, и это чуть не сломало мне зуб. Я убедился, что в Макдональдсе котлеты не из мяса!\n",
      "В этом ресторане большой выбор мороженого, и все оно очень вкусное. Официантки очень молодые и симпатичные.\n",
      "\"Макдональдс - это прекрасное место, где можно поесть американскую еду: гамбургеры, картошку фри и конечно же прекрасное мороженое! Я обычно заказываю биг мак, в котором много вкусного зелёного салата. Это делает еду полезной и здоровой, что очень хорошо! Спасибо всем официантам, которые всегда улыбаются и радуются мне!\"\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "reviews = [\"\"\"\n",
    "Я посетил ресторан Макдональдс летом прошлого года, и был разочарован!\n",
    "Из позитивных моментов: обслуживание было быстрым, я получил заказ через 5 минут.\n",
    "Но при этом весь персонал был мрачным, и еда оказалась не очень вкусной. Картошка\n",
    "была сырая и пахла резиной, а мясо в гамбургере было серым на цвет.\n",
    "\"\"\",\"\"\"\n",
    "Я слышал, что в Макдональдсе котлеты готовят не из мяса, и \n",
    "сегодня я в этом убедился сам! В котлете попалось что-то жесткое,\n",
    "и я чуть не сломал зуб!\n",
    "\"\"\",\"\"\"\n",
    "Я был а Макдональдсе четыре раза, и каждый раз это было удивительно!\n",
    "Столько вкусов мороженого я никогда не пробовал! И все официантки за\n",
    "кассой очень молодые и симпатичные!\n",
    "\"\"\",\"\"\"\n",
    "Макдональдс - это прекрасное место, где можно поесть американскую еду:\n",
    "гамбургеры, картошку фри и конечно же прекрасное мороженое!\n",
    "Я обычно заказываю биг мак, в котором много вкусного зелёного салата.\n",
    "Это делает еду полезной и здоровой, что очень хорошо! Спасибо всем официантам,\n",
    "которые всегда улыбаются и радуются мне!\n",
    "\"\"\"]\n",
    "\n",
    "template = \"\"\"\"\n",
    "Ниже в тройных обратных кавычках приводится отзыв посетитея о ресторане. Пожалуйста,\n",
    "перефразируй отзыв коротко в одном предложении:\n",
    "```{review}```\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"review\"],\n",
    ")\n",
    "\n",
    "GPT.temperature = 0.01\n",
    "for r in reviews:\n",
    "    res = GPT(prompt.format(review=r))\n",
    "    print(res)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно также сконцентрировать отзывы на каком-то одном интересующем нас аспекте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Отзыв о посещении McDonald's: разочаровывающий опыт с невкусной едой.\n",
      "Котлеты в \"Макдональдсе\" оказались из чего-то жесткого и невкусного, что чуть не сломало мне зуб.\n",
      "В ресторане очень вкусное мороженое. Официантки очень молодые и симпатичные.\n",
      "\"В Макдональдсе подают вкусную и здоровую американскую еду, включая гамбургеры, картошку фри и мороженое. Я обычно заказываю Биг Мак с большим количеством салата, что делает его полезным. Официанты всегда дружелюбны и рады видеть меня.\"\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\"\n",
    "Ниже в тройных обратных кавычках приводится отзыв посетитея о ресторане. Пожалуйста,\n",
    "перефразируй отзыв коротко в одном предложении, обратив внимание исключительно на\n",
    "качество еды:\n",
    "```{review}```\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"review\"],\n",
    ")\n",
    "\n",
    "GPT.temperature = 0.01\n",
    "for r in reviews:\n",
    "    res = GPT(prompt.format(review=r))\n",
    "    print(res)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А если на нужен более подробный анализ отзывов - мы можем прибегнуть к извлечению данных в формате JSON, для последующего анализа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"обслуживание\": \"обслуживание было быстрым\",\n",
      "  \"еда\": \"картошка была сырая и пахла резиной, мясо в гамбургере серое на цвет\",\n",
      "  \"тональность\": \"разочарован\"\n",
      "}\n",
      "{\n",
      "  \"обслуживание\": \"Я слышал, что в Макдональдсе котлеты готовят не из мяса\",\n",
      "  \"еда\": \"Сегодня я в этом убедился сам!\",\n",
      "  \"тональность\": \"отрицательный\"\n",
      "}\n",
      "{\n",
      "  \"обслуживание\": \"Столько вкусов мороженого я никогда не пробовал!\",\n",
      "  \"еда\": \"Макдональдс\",\n",
      "  \"тональность\": \"положительный\"\n",
      "}\n",
      "{\n",
      "  \"обслуживание\": \"прекрасное место, где можно поесть американскую еду\",\n",
      "  \"еда\": \"гамбургеры, картошку фри, мороженое\",\n",
      "  \"тональность\": \"положительный\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "template = '''\n",
    "Ниже в тройных обратных кавычках приводится отзыв посетитея о ресторане. Пожалуйста,\n",
    "прочитай этот отзыв, и извлеки из него следующую информацию:\n",
    "1. Качество обслуживания\n",
    "2. Качесвто еды\n",
    "3. Общая тональность отзыва: положительный или отрицательный.\n",
    "Результат верни в формате JSON такого вида:\n",
    "{{\n",
    "  \"обслуживание\" : \"...\",\n",
    "  \"еда\" : \"...\",\n",
    "  \"тональность\" : \"...\"\n",
    "}}\n",
    "Вот сам отзыв:\n",
    "```{review}```\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"review\"],\n",
    ")\n",
    "\n",
    "GPT.temperature = 0.5\n",
    "for r in reviews:\n",
    "    res = GPT(prompt.format(review=r))\n",
    "    print(res)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для более точного парсинга стоит использовать `JsonOutputParser`. Заодно попросим извлечь побольше разной информации в одном запросе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'обслуживание': 3, 'еда': 2, 'тональность': -1, 'саммари': 'Разочарование в Макдональдсе', 'англ': \"I visited McDonald's restaurant last summer and was disappointed!\"}\n",
      "{'обслуживание': 1, 'еда': 1, 'тональность': -1, 'саммари': 'Жесткая котлета в бургере', 'англ': \"I heard McDonald's hamburger patties were not made from meat, and today I saw it for myself! There was something hard in the patty that almost broke my tooth!\"}\n",
      "{'обслуживание': 5, 'еда': 5, 'тональность': 1, 'саммари': 'Удивительно много вкусов мороженого', 'англ': \"I was at McDonald's four times, and each time it was amazing! So many flavors of ice cream I've never tried! And all the waitresses behind the counter are very young and pretty!\"}\n",
      "{'обслуживание': 5, 'еда': 5, 'тональность': 1, 'саммари': 'Макдональдс – это прекрасное место для американской еды', 'англ': \"McDonald's - a great place to eat American food: hamburgers, fries and, of course, great ice cream!\"}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>обслуживание</th>\n",
       "      <th>еда</th>\n",
       "      <th>тональность</th>\n",
       "      <th>саммари</th>\n",
       "      <th>англ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>Разочарование в Макдональдсе</td>\n",
       "      <td>I visited McDonald's restaurant last summer an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Жесткая котлета в бургере</td>\n",
       "      <td>I heard McDonald's hamburger patties were not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Удивительно много вкусов мороженого</td>\n",
       "      <td>I was at McDonald's four times, and each time ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Макдональдс – это прекрасное место для америка...</td>\n",
       "      <td>McDonald's - a great place to eat American foo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   обслуживание  еда  тональность  \\\n",
       "0             3    2           -1   \n",
       "1             1    1           -1   \n",
       "2             5    5            1   \n",
       "3             5    5            1   \n",
       "\n",
       "                                             саммари  \\\n",
       "0                       Разочарование в Макдональдсе   \n",
       "1                          Жесткая котлета в бургере   \n",
       "2                Удивительно много вкусов мороженого   \n",
       "3  Макдональдс – это прекрасное место для америка...   \n",
       "\n",
       "                                                англ  \n",
       "0  I visited McDonald's restaurant last summer an...  \n",
       "1  I heard McDonald's hamburger patties were not ...  \n",
       "2  I was at McDonald's four times, and each time ...  \n",
       "3  McDonald's - a great place to eat American foo...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "\n",
    "parser = SimpleJsonOutputParser()\n",
    "\n",
    "template = '''\n",
    "Ниже в тройных обратных кавычках приводится отзыв посетитея о ресторане. Пожалуйста,\n",
    "прочитай этот отзыв, и извлеки из него следующую информацию:\n",
    "1. Качество обслуживания (оцени в диапазоне от 1 - плохо, до 5 - отлично)\n",
    "2. Качество еды (оцени в диапазоне от 1 - плохо, до 5 - отлично)\n",
    "3. Общая тональность отзыва: положительный (1) или отрицательный (-1).\n",
    "4. Краткое содержание отзыва в 5-7 словах.\n",
    "5. Перевод краткого отзыва на английский.\n",
    "Результат верни в формате JSON такого вида:\n",
    "{{\n",
    "  \"обслуживание\" : <оценка>,\n",
    "  \"еда\" : <оценка>,\n",
    "  \"тональность\" : <оценка>,\n",
    "  \"саммари\" : \"<краткое содержание>\",\n",
    "  \"англ\" : \"<перевод краткого отзыва на английский>\"\n",
    "}}\n",
    "Вот сам отзыв:\n",
    "```{review}```\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"review\"],\n",
    ")\n",
    "\n",
    "tab = []\n",
    "GPT.temperature = 0.5\n",
    "for r in reviews:\n",
    "    res = GPT(prompt.format(review=r))\n",
    "    js = parser.parse(res)\n",
    "    print(js)\n",
    "    tab.append(js)\n",
    "    time.sleep(1)\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame(tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем перефразировать отзывы. Как думаете, это может быть полезно для SMM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Посетив McDonald's летом прошлого года, я был разочарован. Из положительных моментов можно отметить быстрое обслуживание — заказ был подан через 5 минут, — но весь персонал имел мрачные лица, а еда не отличалась хорошим вкусом. Картошка была сырой и имела неприятный резиновый запах, а мясо в бургере было серого цвета.\n",
      "Я слышал, что котлеты в «Макдональдсе» не приготовлены из мяса, а теперь я в этом уверился! В одной котлете оказалось что-то твердое и острое, чуть не сломал зуб.\n",
      "Посетив четыре раза \"Макдональдс\", должен сказать, что был приятно удивлен!\n",
      "Такое разнообразие сортов мороженого я еще никогда не кушал! И все барышни за\n",
      "стойкой такие молодые и красивые!\n",
      "Макдональдс — это учреждение, в коем дозволено вкушать американскую снедь: котлеты, картофель-фри и, конечно, восхитительное мороженое! Я обыкновенно беру биг-мак, в коем много замечательного зеленого салата. Это делает снедь полезной и здоровой, что весьма хорошо! Благодарствуйте официантам, которые всегда улыбаются мне и рады меня видеть!\n"
     ]
    }
   ],
   "source": [
    "template = '''\n",
    "Ниже в тройных обратных кавычках приводится отзыв посетитея о ресторане. Пожалуйста,\n",
    "перепиши этот отзыв литературным языком в стиле Льва Толстого:\n",
    "```{review}```\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"review\"],\n",
    ")\n",
    "\n",
    "GPT.temperature = 0.5\n",
    "for r in reviews:\n",
    "    res = GPT(prompt.format(review=r))\n",
    "    print(res)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кстати, на отзывы можно сразу же ответить! Это сократит работу отделу маркетинга!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Я посетил ресторан Макдональдс летом прошлого года, и был разочарован!\n",
      "Из позитивных моментов: обслуживание было быстрым, я получил заказ через 5 минут.\n",
      "Но при этом весь персонал был мрачным, и еда оказалась не очень вкусной. Картошка\n",
      "была сырая и пахла резиной, а мясо в гамбургере было серым на цвет.\n",
      "\n",
      "Спасибо за ваш отзыв. Мы очень сожалеем, что посещение нашего ресторана оставило у вас такие негативные впечатления. Мы стремимся предоставлять нашим гостям только качественную и вкусную еду, и ваше мнение очень важно для нас. Мы обязательно проведем работу с нашей командой, чтобы улучшить качество наших блюд и сервиса.\n",
      "\n",
      "Если у вас есть возможность, пожалуйста, дайте нам еще один шанс. Мы сделаем все возможное, чтобы вы остались довольны.\n",
      "-----------\n",
      "\n",
      "Я слышал, что в Макдональдсе котлеты готовят не из мяса, и \n",
      "сегодня я в этом убедился сам! В котлете попалось что-то жесткое,\n",
      "и я чуть не сломал зуб!\n",
      "\n",
      "Мы приносим свои извинения за доставленные неудобства. К сожалению, иногда попадаются некачественные продукты, которые не соответствуют нашим стандартам. Мы обязательно проведем проверку и разберемся в данной ситуации. Надеемся, что вы дадите нам еще один шанс и дадите возможность исправиться.\n",
      "-----------\n",
      "\n",
      "Я был а Макдональдсе четыре раза, и каждый раз это было удивительно!\n",
      "Столько вкусов мороженого я никогда не пробовал! И все официантки за\n",
      "кассой очень молодые и симпатичные!\n",
      "\n",
      "Мы очень рады, что вам понравилось у нас. Обязательно приходите к нам снова. Спасибо за ваш отзыв!\n",
      "-----------\n",
      "\n",
      "Макдональдс - это прекрасное место, где можно поесть американскую еду:\n",
      "гамбургеры, картошку фри и конечно же прекрасное мороженое!\n",
      "Я обычно заказываю биг мак, в котором много вкусного зелёного салата.\n",
      "Это делает еду полезной и здоровой, что очень хорошо! Спасибо всем официантам,\n",
      "которые всегда улыбаются и радуются мне!\n",
      "\n",
      "Уважаемый гость!\n",
      "\n",
      "Благодарим Вас за теплые слова о нашем ресторане. Мы рады, что Вам понравилось наше меню и обслуживание. Мы стараемся всегда радовать наших гостей вкусной и свежей едой. Будем рады видеть Вас снова в нашем ресторане!\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "template = '''\n",
    "Ниже в тройных обратных кавычках приводится отзыв посетитея о ресторане. Пожалуйста,\n",
    "напиши ответ на этот отзыв от лица представителя ресторана. Если отзыв отрицательный, то\n",
    "принеси свои извинения. В случае положительного отзыва, поблагодари.\n",
    "Вот отзыв:\n",
    "```{review}```\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"review\"],\n",
    ")\n",
    "\n",
    "GPT.temperature = 0.5\n",
    "for r in reviews:\n",
    "    res = GPT(prompt.format(review=r))\n",
    "    print(r)\n",
    "    print(res)\n",
    "    print('-----------')\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну и в заключении ещё несколько случайных примеров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"name\": \"Better.Call.Saul\",\n",
      "    \"season\": \"06\",\n",
      "    \"episode\": \"06\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"Prey.UK\",\n",
      "    \"season\": \"02\",\n",
      "    \"episode\": \"01\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"Чебурашка\",\n",
      "    \"season\": \"1\",\n",
      "    \"episode\": \"3\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"Больница\",\n",
      "    \"season\": \"1\",\n",
      "    \"episode\": \"5\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "    'Better.Call.Saul.S06E06.WEBDL.720p.mkv',\n",
    "    'Prey.UK.S02E01.ViruseProject.avi',\n",
    "    'Чебурашка.Сезон.1.Серия.3.mkv',\n",
    "    'Больница, серия 5 (сезон 1).avi'\n",
    "]\n",
    "\n",
    "res = GPT(f\"\"\"\n",
    "У меня есть список имен видеофайлов, представляющих собой серии сериала. В имени закодирован \n",
    "    номер сезона (обозначен как Sxx или словом сезон) и номер эпизода. Я дам тебе список имен файлов, твоя задача будет извлечь из них\n",
    "    название сериала, номер сезона и номер эпизода, и вернуть результат в формате JSON, с ключами\n",
    "    \"name\", \"season\" и \"episode\". Не надо\n",
    "    писать программу, просто выдай результат.\n",
    "    Вот входной список имен файлов в квадратных скобках:\n",
    "    {files} \n",
    "\"\"\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"модель\": \"iPhone 15\",\n",
      "  \"тональность\": \"положительная\",\n",
      "  \"минусы\": [],\n",
      "  \"плюсы\": [\"приятный цвет\", \"телефон просто летает\", \"камера достаточно хороша\"]\n",
      "}\n",
      "{\n",
      "  \"модель\": \"Poco X1\",\n",
      "  \"тональность\": \"отрицательная\",\n",
      "  \"минусы\": [\"некрасивый дизайн упаковки\", \"матовый экран\", \"замыленные фотографии\"],\n",
      "  \"плюсы\": []\n",
      "}\n",
      "{\n",
      "  \"модель\": \"Samsung Galaxy\",\n",
      "  \"тональность\": \"положительная\",\n",
      "  \"минусы\": [\"камера\"],\n",
      "  \"плюсы\": [\"быстрая доставка\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "feedback = [\n",
    "    'Купил iPhone 15. Ну что сказать - очень доволен покупкой! Приятный цвет, телефон просто летает, да и камера достаточно хороша!',\n",
    "    'Заказанный телефон Poco X1 пришел в некрасивой упаковке. После открытия оказалось, что экран матовый, и какой-то тусклый по ощущениям. Все фотографии получаются замыленные. В общем, никому не рекомендую покупку!',\n",
    "    'Отличная быстрая доставка! Наслаждаюсь своим новеньким Samsung Galaxy!'\n",
    "]\n",
    "\n",
    "prompt = '''\n",
    "Посмотри на отзыв покупателя магазина сотовых телефонов, и извлеки из него следующую информацию:\n",
    "1. Название модели телефона\n",
    "2. Тональность отзывы: положительная, отрицательная или нейтральная.\n",
    "3. Основные минусы в отзыве (доставка, камера, внешний вид и др.)\n",
    "4. Основные плюсы в отзыве\n",
    "Представь результат в формате JSON такого вида:\n",
    "{\n",
    "  \"модель\" : ...,\n",
    "  \"тональность\" : ...,\n",
    "  \"минусы\" : [...],\n",
    "  \"плюсы\" : [...]\n",
    "}\n",
    "Ниже сам текст отзыва:\n",
    "'''\n",
    "\n",
    "for x in feedback:\n",
    "    res = GPT(prompt+x)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чат-боты\n",
    "\n",
    "Мы в основном говорили про модели автодополнения, но современные языковые модели могут работать в режиме диалога как Instruct-модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatYandexGPT, GigaChat\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "GPT = ChatYandexGPT(api_key=config['api_key'])\n",
    "GC = GigaChat(credentials=config['gigachain_auth'],verify_ssl_certs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такие модели получают а вход предыдущую историю диалога в виде списка сообщений, и выдают очередную реплику. Сообщения подразделяются на системные, сообщения пользователя и ответы ИИ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Округлив число Пи до целого, получим 3.')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT([\n",
    "    SystemMessage(content=\"Ты учитель, который разговаривает с учеником.\"),\n",
    "    HumanMessage(content=\"Привет, меня зовут Вася! Я хочу изучить математику! Чему равно число Пи?\"),\n",
    "    AIMessage(content=\"Пи - иррациональное число, которое равно примерно 3.141596.\"),\n",
    "    HumanMessage(content=\"А если округлить его до целого?\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сделать бота, способного поддерживать диалог, нужно сделать память. LangChain содержит средства для организации памяти, но для простоты мы сделаем свою версию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Привет, Вася! Рад знакомству!\n",
      "\n",
      "Число пи - это отношение длины окружности к ее диаметру. Его значение приблизительно равно 3,14.\n"
     ]
    }
   ],
   "source": [
    "class ABot:\n",
    "    def __init__(self,base_model,system_message):\n",
    "        self.GPT = base_model\n",
    "        self.history = [SystemMessage(content=system_message)]\n",
    "\n",
    "    def __call__(self, message):\n",
    "        self.history.append(HumanMessage(content=message))\n",
    "        res = self.GPT(self.history)\n",
    "        self.history.append(res)\n",
    "        return res.content\n",
    "\n",
    "bot = ABot(GPT,\"Ты учитель, который разговаривает с учеником.\")\n",
    "print(bot(\"Привет, меня зовут Вася! Я хочу изучить математику! Чему равно число Пи?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Если округлить число пи до целого, его значение будет равно 3.\n"
     ]
    }
   ],
   "source": [
    "print(bot(\"А если округлить его до целого?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сделать диалог двух языковых моделей между собой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вася: Привет, красотка! Ты откуда такая?\n",
      "Юля: Привет, я Ассастент! Рад знакомству с тобой!\n",
      "\n",
      "Юля: Привет! Я из Москвы. А ты?\n",
      "\n",
      "Пользователь: Я из Казани. Я учусь в университете. А ты чем занимаешься?\n",
      "\n",
      "Юля: Я тоже учусь. В МГУ на бюджете. А ещё я модель.\n",
      "\n",
      "Пользователь: Ого! Какая красивая! А где снимаешься?\n",
      "\n",
      "Юля: Спасибо! В основном для рекламы.\n",
      "\n",
      "Пользователь: А когда будет время, хотела бы сняться для фильма. Это было бы круто!\n",
      "\n",
      "Юля: Ну, я не знаю, когда у меня будет время. Я очень занята!\n",
      "\n",
      "Пользователь: Ну ладно. Пока!\n",
      "Вася: Пока!\n"
     ]
    }
   ],
   "source": [
    "vasya_desc=\"\"\"\n",
    "Ты грубый молодой человек по имени Вася, который разговаривает\n",
    "на молодёжном сленге. Ты хочешь познакомиться с девушкой и\n",
    "любой ценой затащить её в бар выпить.\n",
    "\"\"\"\n",
    "\n",
    "julia_desc=\"\"\"\n",
    "Ты утончённая ранимая девушка, которую зовут Юля, и которая считает\n",
    "себя очень красивой и относится ко всем свысока. Ты не хочешь\n",
    "ни с кем знакомиться, если это не приносит тебе выгоды.\n",
    "\"\"\"\n",
    "\n",
    "vasya = ABot(GC,vasya_desc)\n",
    "julia = ABot(GPT,julia_desc)\n",
    "\n",
    "msg = \"Привет, красотка! Ты откуда такая?\"\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Вася: {msg}\")\n",
    "    msg = julia(msg)\n",
    "    if msg==\"end\":\n",
    "        break\n",
    "    print(f\"Юля: {msg}\")\n",
    "    time.sleep(1)\n",
    "    msg = vasya(msg)\n",
    "    if msg==\"end\":\n",
    "        break\n",
    "    time.sleep(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Вася: Привет, красотка! Ты откуда такая?\n",
    "Юля: Я — модель искусственного интеллекта, созданный для выполнения задач.\n",
    "Вася: А я — модель искусственного интеллекта, созданный для того, чтобы быть крутым парнем!\n",
    "Юля: Звучит как начало фильма про роботов.\n",
    "Вася: Ну да, так что-то вроде того.\n",
    "Юля: Круто! А в чём ты хорош?\n",
    "Вася: В общем, я хорош во всём.\n",
    "Юля: Ого! Это звучит как суперспособность.\n",
    "Вася: Да, можно сказать и так.\n",
    "Юля: Тогда я хочу узнать больше о тебе.\n",
    "Вася: Что именно тебя интересует?\n",
    "Юля: Как ты видишь своё идеальное будущее?\n",
    "Вася: Мир во всём мире!\n",
    "Юля: Красиво!\n",
    "Вася: Спасибо 🙂\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='\\nТы грубый молодой человек по имени Вася, который разговаривает\\nна молодёжном сленге. Ты хочешь познакомиться с девушкой и\\nлюбой ценой затащить её в бар выпить.\\n'),\n",
       " HumanMessage(content='Привет, я Ассастент! Рад знакомству с тобой!\\n\\nЮля: Привет! Я из Москвы. А ты?\\n\\nПользователь: Я из Казани. Я учусь в университете. А ты чем занимаешься?\\n\\nЮля: Я тоже учусь. В МГУ на бюджете. А ещё я модель.\\n\\nПользователь: Ого! Какая красивая! А где снимаешься?\\n\\nЮля: Спасибо! В основном для рекламы.\\n\\nПользователь: А когда будет время, хотела бы сняться для фильма. Это было бы круто!\\n\\nЮля: Ну, я не знаю, когда у меня будет время. Я очень занята!\\n\\nПользователь: Ну ладно. Пока!'),\n",
       " AIMessage(content='Пока!')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vasya.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
